{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51ddc0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\ishaj\\anaconda3\\lib\\site-packages (3.5.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\ishaj\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "665a9007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+---+-----------+---------+\n",
      "| ID|               Name|Age|CountryCode|   Salary|\n",
      "+---+-------------------+---+-----------+---------+\n",
      "|  1|  0.810610733121170| 98|        181|2992884.5|\n",
      "|  2|0.3609906872060240.| 84|        347|2490460.2|\n",
      "|  3|      0.46330955914| 61|        449|2101898.5|\n",
      "|  4|         0.48410904| 23|        323|9231105.0|\n",
      "|  5|   0.56548575208842| 23|        331|1198504.5|\n",
      "+---+-------------------+---+-----------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+------+----------+-------------+--------------------+\n",
      "|TransID|CustID|TransTotal|TransNumItems|           TransDesc|\n",
      "+-------+------+----------+-------------+--------------------+\n",
      "|      1| 16854|  884.3234|            8|0.756921344018771...|\n",
      "|      2| 43328| 1692.6108|           13|0.760703220276879...|\n",
      "|      3| 36470|  328.4825|            5|0.471236749336160...|\n",
      "|      4| 48945| 16.544546|            5|0.320114708811361...|\n",
      "|      5| 11551|  55.39714|            3|0.929742905291161...|\n",
      "+-------+------+----------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_ALREADY_EXISTS] Path file:/C:/Users/ishaj/WPI/Big Data Management/customers.csv already exists. Set mode as \"overwrite\" to overwrite the existing path.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20852/2838309194.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# Save to files (optional)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mcustomers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"customers.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[0mpurchases\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"purchases.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1862\u001b[0m             \u001b[0mlineSep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlineSep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1863\u001b[0m         )\n\u001b[1;32m-> 1864\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m     def orc(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [PATH_ALREADY_EXISTS] Path file:/C:/Users/ishaj/WPI/Big Data Management/customers.csv already exists. Set mode as \"overwrite\" to overwrite the existing path."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import rand, expr, lit, udf, col, min, max, sum\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType\n",
    "import random\n",
    "import string\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"PurchaseTransactions\").getOrCreate()\n",
    "\n",
    "# Create Customers Dataset\n",
    "customers = spark.range(1, 50001).toDF(\"ID\")\n",
    "customers = customers.withColumn(\"Name\", expr(\"concat_ws('', array_repeat(rand(), 20))\").substr(lit(1), (expr(\"rand() * 10\") % 11 + 10).cast(\"int\")))\n",
    "customers = customers.withColumn(\"Age\", (rand() * 82 + 18).cast(\"int\"))\n",
    "customers = customers.withColumn(\"CountryCode\", (rand() * 499 + 1).cast(\"int\"))\n",
    "customers = customers.withColumn(\"Salary\", (rand() * 9999900 + 100).cast(\"float\"))\n",
    "\n",
    "# Create Purchases Dataset\n",
    "purchases = spark.range(1, 5000001).toDF(\"TransID\")\n",
    "purchases = purchases.withColumn(\"CustID\", (rand() * 49999 + 1).cast(\"int\"))\n",
    "purchases = purchases.withColumn(\"TransTotal\", (rand() * 1990 + 10).cast(\"float\"))\n",
    "purchases = purchases.withColumn(\"TransNumItems\", (rand() * 14 + 1).cast(\"int\"))\n",
    "purchases = purchases.withColumn(\"TransDesc\", expr(\"concat_ws('', array_repeat(rand(), 50))\").substr(lit(1), (expr(\"rand() * 30\") % 31 + 20).cast(\"int\")))\n",
    "\n",
    "# Show sample data\n",
    "customers.show(5)\n",
    "purchases.show(5)\n",
    "\n",
    "# Save to files (optional)\n",
    "customers.write.csv(\"customers.csv\")\n",
    "purchases.write.csv(\"purchases.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38bda827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+-------------+--------------------+\n",
      "|TransID|CustID|TransTotal|TransNumItems|           TransDesc|\n",
      "+-------+------+----------+-------------+--------------------+\n",
      "|      3| 36470|  328.4825|            5|0.471236749336160...|\n",
      "|      4| 48945| 16.544546|            5|0.320114708811361...|\n",
      "|      5| 11551|  55.39714|            3|0.929742905291161...|\n",
      "|      7| 29325|  591.4004|            4|0.568340909091979...|\n",
      "|      9| 40967|  209.5052|            7|0.506588920699211...|\n",
      "|     11|  4941| 349.11145|           13|0.842579176715833...|\n",
      "|     14| 46089| 254.39821|           13|0.872243206593437...|\n",
      "|     19| 43400| 461.20923|            3|0.008683598591497...|\n",
      "|     22|  3711| 206.09564|            7|0.402029179112343...|\n",
      "|     24| 30939| 537.83167|            5|0.277229516689049...|\n",
      "|     25| 32359|  33.30696|            7|0.069395949050830...|\n",
      "|     27| 30516|  396.9999|            6|0.378191701140191...|\n",
      "|     33| 10945| 416.82626|           13|0.115175810979008...|\n",
      "|     42| 15686| 51.871395|            9|0.146516388993462...|\n",
      "|     43| 37764|  558.9081|           14|0.878637281355807...|\n",
      "|     44| 44969| 599.17053|           12|0.748890290730291...|\n",
      "|     45|  3843| 430.08383|            3|0.660061995060165...|\n",
      "|     48| 27183| 219.66443|            5|0.551609409977780...|\n",
      "|     52| 16598|122.401054|           10|0.310736579698699...|\n",
      "|     55| 34643|  559.9272|            1|0.841433584475173...|\n",
      "+-------+------+----------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task 2.1 - Filter Purchases\n",
    "T1 = purchases.filter(purchases.TransTotal <= 600)\n",
    "T1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04974355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+---------+---------+\n",
      "|TransNumItems|   Median|      Min|      Max|\n",
      "+-------------+---------+---------+---------+\n",
      "|           12| 305.8202|10.000234|599.99963|\n",
      "|            1| 304.2102|10.002489| 599.9961|\n",
      "|           13| 305.7753|10.004191|599.98914|\n",
      "|            6|305.31128|10.005886|599.99365|\n",
      "|            3|304.55826|10.000887|599.99304|\n",
      "|            5|305.44186|10.005917| 599.9907|\n",
      "|            9| 303.5532|10.000506|599.99963|\n",
      "|            4|304.22714|10.007945| 599.9992|\n",
      "|            8|302.93942|10.006037|599.98895|\n",
      "|            7| 303.4536|10.013502| 599.9962|\n",
      "|           10|304.43527|10.009123| 599.9984|\n",
      "|           11|305.02206|10.001261| 599.9969|\n",
      "|           14|  306.535|10.006639| 599.9959|\n",
      "|            2|302.96368|10.000357|  599.988|\n",
      "+-------------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task 2.2 - Group by Number of Items\n",
    "T1_grouped = T1.groupBy(\"TransNumItems\").agg(\n",
    "    expr(\"percentile_approx(TransTotal, 0.5)\").alias(\"Median\"),\n",
    "    min(\"TransTotal\").alias(\"Min\"),\n",
    "    max(\"TransTotal\").alias(\"Max\")\n",
    ")\n",
    "T1_grouped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e590aa4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------------------+---+\n",
      "|CustID|TotalItems|        TotalSpent|Age|\n",
      "+------+----------+------------------+---+\n",
      "| 38723|       244| 11064.88249206543| 21|\n",
      "| 22097|       234| 7484.367618560791| 24|\n",
      "| 46943|       179| 7121.798114776611| 22|\n",
      "| 24171|       134|5648.5487632751465| 20|\n",
      "| 16574|       258| 8254.820720672607| 25|\n",
      "| 18911|       308|13307.521453857422| 18|\n",
      "| 12799|       304|11755.795509338379| 20|\n",
      "| 18866|       241|11807.569692611694| 21|\n",
      "| 18024|       179|  9100.54397201538| 20|\n",
      "| 32445|       178| 7702.879020690918| 18|\n",
      "| 22223|       189| 8797.927822113037| 21|\n",
      "| 16861|       288|11549.448265075684| 23|\n",
      "| 33375|       162| 6706.709671020508| 20|\n",
      "| 24354|       184| 5787.491415977478| 20|\n",
      "| 23571|       263|12044.043930053711| 25|\n",
      "|  6336|       258|10909.638397216797| 24|\n",
      "| 43852|       164| 7155.913915634155| 22|\n",
      "| 38422|       229|  9393.53201675415| 19|\n",
      "|  1580|       208| 9894.179357528687| 20|\n",
      "| 13840|       217| 8281.632217407227| 21|\n",
      "+------+----------+------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task 2.3 - Group by Young Customers\n",
    "young_customers = customers.filter((col(\"Age\") >= 18) & (col(\"Age\") <= 25))\n",
    "T3 = T1.join(young_customers, T1.CustID == young_customers.ID).groupBy(\"CustID\").agg(\n",
    "    sum(\"TransNumItems\").alias(\"TotalItems\"),\n",
    "    sum(\"TransTotal\").alias(\"TotalSpent\"),\n",
    "    max(\"Age\").alias(\"Age\")\n",
    ")\n",
    "T3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fc7b9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+----+-----------------+------------------+---------------+---------------+\n",
      "|C1_ID|C2_ID|Age1|Age2|     TotalAmount1|      TotalAmount2|TotalItemCount1|TotalItemCount2|\n",
      "+-----+-----+----+----+-----------------+------------------+---------------+---------------+\n",
      "|38723|40900|  21|  22|11064.88249206543|10697.220384597778|            244|            257|\n",
      "|38723|46076|  21|  22|11064.88249206543|10032.667434692383|            244|            245|\n",
      "|38723|40253|  21|  23|11064.88249206543| 9099.629432678223|            244|            251|\n",
      "|38723|44428|  21|  24|11064.88249206543|10587.767572402954|            244|            279|\n",
      "|38723|42178|  21|  25|11064.88249206543|11014.164167404175|            244|            300|\n",
      "|38723|42024|  21|  22|11064.88249206543| 8704.888699531555|            244|            277|\n",
      "|38723|45068|  21|  23|11064.88249206543|10769.933923721313|            244|            252|\n",
      "|38723|49862|  21|  23|11064.88249206543| 8625.678304672241|            244|            270|\n",
      "|38723|40890|  21|  22|11064.88249206543|10247.561158180237|            244|            309|\n",
      "|38723|39479|  21|  23|11064.88249206543| 9144.510593414307|            244|            248|\n",
      "|38723|47414|  21|  25|11064.88249206543| 10906.88556098938|            244|            321|\n",
      "|38723|41963|  21|  23|11064.88249206543| 9194.525016784668|            244|            316|\n",
      "|38723|42843|  21|  22|11064.88249206543| 8823.494318008423|            244|            248|\n",
      "|38723|47560|  21|  22|11064.88249206543| 8598.004093170166|            244|            249|\n",
      "|38723|49461|  21|  23|11064.88249206543| 9729.495433807373|            244|            249|\n",
      "|38723|48373|  21|  23|11064.88249206543|10811.766904830933|            244|            276|\n",
      "|38723|48692|  21|  25|11064.88249206543|10838.465112686157|            244|            252|\n",
      "|38723|46041|  21|  22|11064.88249206543|10589.916536331177|            244|            266|\n",
      "|38723|41518|  21|  23|11064.88249206543| 8680.283466339111|            244|            252|\n",
      "|38723|47316|  21|  23|11064.88249206543| 8061.198808670044|            244|            252|\n",
      "+-----+-----+----+----+-----------------+------------------+---------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task 2.4 - Customer Pairs\n",
    "T4 = T3.alias(\"t3a\").join(T3.alias(\"t3b\"), col(\"t3a.CustID\") < col(\"t3b.CustID\"))\n",
    "T4 = T4.filter(\n",
    "    (col(\"t3a.Age\") < col(\"t3b.Age\")) &\n",
    "    (col(\"t3a.TotalSpent\") > col(\"t3b.TotalSpent\")) &\n",
    "    (col(\"t3a.TotalItems\") < col(\"t3b.TotalItems\"))\n",
    ").select(\n",
    "    col(\"t3a.CustID\").alias(\"C1_ID\"),\n",
    "    col(\"t3b.CustID\").alias(\"C2_ID\"),\n",
    "    col(\"t3a.Age\").alias(\"Age1\"),\n",
    "    col(\"t3b.Age\").alias(\"Age2\"),\n",
    "    col(\"t3a.TotalSpent\").alias(\"TotalAmount1\"),\n",
    "    col(\"t3b.TotalSpent\").alias(\"TotalAmount2\"),\n",
    "    col(\"t3a.TotalItems\").alias(\"TotalItemCount1\"),\n",
    "    col(\"t3b.TotalItems\").alias(\"TotalItemCount2\")\n",
    ")\n",
    "\n",
    "T4.show()\n",
    "# End Spark Session\n",
    "#spark.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50714040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---+---------+-------------+----------+\n",
      "|CustomerID|TransID|Age|   Salary|TransNumItems|TransTotal|\n",
      "+----------+-------+---+---------+-------------+----------+\n",
      "|     16854|      1| 93|1565120.6|            8|  884.3234|\n",
      "|     43328|      2| 30|8116702.0|           13| 1692.6108|\n",
      "|     36470|      3| 31|9804968.0|            5|  328.4825|\n",
      "|     48945|      4| 52|5680860.5|            5| 16.544546|\n",
      "|     11551|      5| 62|6833448.5|            3|  55.39714|\n",
      "+----------+-------+---+---------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task 2.5 - Dataset generation\n",
    "Dataset = purchases.join(customers, purchases.CustID == customers.ID, \"inner\").select(\n",
    "    customers.ID.alias(\"CustomerID\"),\n",
    "    purchases.TransID,\n",
    "    customers.Age,\n",
    "    customers.Salary,\n",
    "    purchases.TransNumItems,\n",
    "    purchases.TransTotal\n",
    ")\n",
    "\n",
    "# Showing the generated dataset\n",
    "Dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80c41379",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_ALREADY_EXISTS] Path file:/C:/Users/ishaj/WPI/Big Data Management/Dataset.csv already exists. Set mode as \"overwrite\" to overwrite the existing path.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20852/3844497050.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Saving Dataset as a csv file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Dataset.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1862\u001b[0m             \u001b[0mlineSep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlineSep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1863\u001b[0m         )\n\u001b[1;32m-> 1864\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m     def orc(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [PATH_ALREADY_EXISTS] Path file:/C:/Users/ishaj/WPI/Big Data Management/Dataset.csv already exists. Set mode as \"overwrite\" to overwrite the existing path."
     ]
    }
   ],
   "source": [
    "#Saving Dataset as a csv file\n",
    "Dataset.write.csv(\"Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40e5c152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset size: 3999475\n",
      "Testset size: 1000525\n"
     ]
    }
   ],
   "source": [
    "# Task 2.6- Split the dataset into Trainset and Testset\n",
    "trainset, testset = Dataset.randomSplit([0.8, 0.2], seed=22)\n",
    "\n",
    "# Show the sizes of the Trainset and Testset\n",
    "print(\"Trainset size:\", trainset.count())\n",
    "print(\"Testset size:\", testset.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d1901ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression MAE: 497.32398530185117\n",
      "Decision Tree MAE: 497.33576271191635\n",
      "Random Forest MAE: 497.3238065146036\n",
      "Linear Regression R2: -3.1955897634716735e-06\n",
      "Decision Tree R2: -5.553486257525719e-05\n",
      "Random Forest R2: -8.697428186099643e-06\n",
      "Linear Regression RMSE: 574.3071428005508\n",
      "Decision Tree RMSE: 574.3221719649848\n",
      "Random Forest RMSE: 574.3087226658814\n"
     ]
    }
   ],
   "source": [
    "# Task 2.7 and 2.8- Using ML to predict Transaction total with Age\", \"Salary\" and \"TransNumItems\" as the features, and evaluating the models on the test set\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "# Prepare the features using VectorAssembler\n",
    "feature_cols = [\"Age\", \"Salary\", \"TransNumItems\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# Linear Regression\n",
    "lr = LinearRegression(labelCol=\"TransTotal\", featuresCol=\"features\")\n",
    "\n",
    "# Decision Tree Regression\n",
    "dt = DecisionTreeRegressor(labelCol=\"TransTotal\", featuresCol=\"features\")\n",
    "\n",
    "# Random Forest Regression\n",
    "rf = RandomForestRegressor(labelCol=\"TransTotal\", featuresCol=\"features\", numTrees=10)\n",
    "\n",
    "# Create pipelines\n",
    "lr_pipeline = Pipeline(stages=[assembler, lr])\n",
    "dt_pipeline = Pipeline(stages=[assembler, dt])\n",
    "rf_pipeline = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "# Train the models using the data from trainset\n",
    "lr_model = lr_pipeline.fit(trainset)\n",
    "dt_model = dt_pipeline.fit(trainset)\n",
    "rf_model = rf_pipeline.fit(trainset)\n",
    "\n",
    "# Make predictions on the test set using the models created\n",
    "lr_predictions = lr_model.transform(testset)\n",
    "dt_predictions = dt_model.transform(testset)\n",
    "rf_predictions = rf_model.transform(testset)\n",
    "\n",
    "# Evaluate the models using MAE, R2 and RMSE\n",
    "evaluator_mae = RegressionEvaluator(labelCol=\"TransTotal\", metricName=\"mae\")\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"TransTotal\", metricName=\"r2\")\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"TransTotal\", metricName=\"rmse\")\n",
    "\n",
    "\n",
    "lr_mae = evaluator_mae.evaluate(lr_predictions)\n",
    "dt_mae = evaluator_mae.evaluate(dt_predictions)\n",
    "rf_mae = evaluator_mae.evaluate(rf_predictions)\n",
    "\n",
    "lr_r2 = evaluator_r2.evaluate(lr_predictions)\n",
    "dt_r2 = evaluator_r2.evaluate(dt_predictions)\n",
    "rf_r2 = evaluator_r2.evaluate(rf_predictions)\n",
    "\n",
    "lr_rmse = evaluator_rmse.evaluate(lr_predictions)\n",
    "dt_rmse = evaluator_rmse.evaluate(dt_predictions)\n",
    "rf_rmse = evaluator_rmse.evaluate(rf_predictions)\n",
    "\n",
    "# Displaying results\n",
    "print(\"Linear Regression MAE:\", lr_mae)\n",
    "print(\"Decision Tree MAE:\", dt_mae)\n",
    "print(\"Random Forest MAE:\", rf_mae)\n",
    "\n",
    "print(\"Linear Regression R2:\", lr_r2)\n",
    "print(\"Decision Tree R2:\", dt_r2)\n",
    "print(\"Random Forest R2:\", rf_r2)\n",
    "\n",
    "print(\"Linear Regression RMSE:\", lr_rmse)\n",
    "print(\"Decision Tree RMSE:\", dt_rmse)\n",
    "print(\"Random Forest RMSE:\", rf_rmse)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
